{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "\n",
    "font_name = font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font', family=font_name)    \n",
    "matplotlib.rcParams['axes.unicode_minus'] = False      \n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "pd.set_option('display.max_columns', 150)\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "random_seed = 616"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = '유입량'\n",
    "PK_col = ['홍수사상번호','연','월','일','시간']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_excel('../data/01_제공데이터/2021 빅콘테스트_데이터분석분야_퓨처스리그_홍수ZERO_댐유입량,강우,수위데이터_210803.xlsx', header=[0,1])\n",
    "\n",
    "data_raw.columns = ['_'.join(col).strip() if \"Unnamed\" not in col[1] else col[0] for col in data_raw.columns.values]\n",
    "test_idx = data_raw[data_raw.isna().any(axis=1)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 값 저장할 dataframe 생성\n",
    "\n",
    "predict_df = pd.DataFrame()\n",
    "predict_df[PK_col] = data_raw[PK_col]\n",
    "predict_df['true'] = data_raw.유입량\n",
    "\n",
    "score_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer=(1, 1, 1), batch_norm=True, drop_out=True, drop_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.hidden_layer = self.make_layers(input_size, hidden_layer, batch_norm, drop_out, drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.hidden_layer(x)\n",
    "        return out\n",
    "\n",
    "    def make_layers(self, input_size, hidden_layer, batch_norm, drop_out, drop_prob):\n",
    "        layers = []\n",
    "        in_size = input_size\n",
    "        if hidden_layer[0] == 'R':\n",
    "            for v in hidden_layer[1:]:\n",
    "                out_size = int(v)\n",
    "                linear = nn.Linear(in_size, out_size)\n",
    "                torch.nn.init.xavier_uniform_(linear.weight)\n",
    "                layers += [linear]\n",
    "\n",
    "                if batch_norm:\n",
    "                    layers += [nn.BatchNorm1d(out_size)]\n",
    "\n",
    "                if drop_out:\n",
    "                    layers += [torch.nn.Dropout(p=drop_prob)]\n",
    "\n",
    "                layers += [nn.ReLU(inplace=True)]\n",
    "\n",
    "                in_size = out_size\n",
    "        else:\n",
    "            for v in hidden_layer:\n",
    "                out_size = int(input_size * v)\n",
    "                linear = nn.Linear(in_size, out_size)\n",
    "                torch.nn.init.xavier_uniform_(linear.weight)\n",
    "                layers += [linear]\n",
    "\n",
    "                if batch_norm:\n",
    "                    layers += [nn.BatchNorm1d(out_size)]\n",
    "\n",
    "                if drop_out:\n",
    "                    layers += [torch.nn.Dropout(p=drop_prob)]\n",
    "\n",
    "                layers += [nn.ReLU(inplace=True)]\n",
    "\n",
    "                in_size = out_size\n",
    "\n",
    "        linear = nn.Linear(out_size, 1)\n",
    "        torch.nn.init.xavier_uniform_(linear.weight)\n",
    "        layers += [linear]\n",
    "\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "robust_scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_values = data_raw.groupby('홍수사상번호').first()['유입량']\n",
    "\n",
    "def dnn_predict(name, data, model_structure, scaler, view=False, view_num=5, early_stop_num = 3):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device\n",
    "    pred_values = []\n",
    "    predict_df = pd.DataFrame()\n",
    "    predict_df[PK_col] = data[['홍수사상번호','연','월','일','시간']]\n",
    "    predict_df['true'] = data.유입량\n",
    "    \n",
    "    for num in data.홍수사상번호.unique():\n",
    "        print(f\"홍수사상번호 {num} 시작\")\n",
    "        train = data[data['홍수사상번호'] != num].dropna().copy()\n",
    "        valid = data[data['홍수사상번호'] == num].copy()\n",
    "\n",
    "        X_train = train.drop(columns=[y_col]+PK_col)\n",
    "        y_train = train[y_col]\n",
    "\n",
    "        X_valid = valid.drop(columns=[y_col]+PK_col)\n",
    "        y_valid = valid[y_col]\n",
    "        \n",
    "        scaler.fit(X_train)\n",
    "\n",
    "        X_train_tensors = torch.Tensor(scaler.transform(X_train)).to(device)\n",
    "        X_valid_tensors = torch.Tensor(scaler.transform(X_valid)).to(device)\n",
    "\n",
    "        y_train_tensors = torch.Tensor(y_train.values).to(device)\n",
    "        y_valid_tensors = torch.Tensor(y_valid.values).to(device)\n",
    "        \n",
    "        input_size = X_train.shape[1]\n",
    "\n",
    "        dnn = DNN(input_size, model_structure[\"hidden_layer\"], model_structure[\"batch_norm\"],\n",
    "                  model_structure[\"drop_out\"], model_structure[\"drop_prob\"]).to(device)\n",
    "        criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "        optimizer = torch.optim.Adam(dnn.parameters(), lr=model_structure[\"learning_rate\"])  # adam optimizer\n",
    "        \n",
    "        best_loss = 1E+10\n",
    "        best_epoch = 0\n",
    "        for epoch in range(model_structure[\"num_epochs\"]):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = dnn(X_train_tensors)\n",
    "            loss = criterion(outputs.view(-1), y_train_tensors.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                dnn.eval()\n",
    "                outputs_valid = dnn(X_valid_tensors)\n",
    "                loss_valid = criterion(outputs_valid.view(-1), y_valid_tensors.view(-1))\n",
    "                if loss_valid < best_loss:\n",
    "                    best_loss = loss_valid\n",
    "                    best_epoch = epoch\n",
    "                    pred_valid = list(outputs_valid.view(-1).detach().cpu().numpy())\n",
    "                dnn.train()\n",
    "\n",
    "            if view and ((epoch+1) % (model_structure[\"num_epochs\"]//view_num) == 0):\n",
    "                print(\"  Epoch: %d, Train Loss: %1.2f Best Epoch: %d Best Valid Loss: %1.2f\" % (epoch+1, loss.item(), best_epoch+1, best_loss.item()))\n",
    "            \n",
    "            if (epoch - best_epoch) > (model_structure[\"num_epochs\"] // early_stop_num):\n",
    "                print(\"Best Epoch: %d Best Valid Loss: %1.2f\" % (best_epoch+1, best_loss.item()))\n",
    "                break\n",
    "        print(\"\")\n",
    "        pred_values += pred_valid\n",
    "    \n",
    "    predict_df[f'pred_DNN_{name}'] = pred_values\n",
    "    \n",
    "    return predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan(predict_df):\n",
    "    for num in predict_df.홍수사상번호:\n",
    "        predict_num = predict_df[predict_df.홍수사상번호 == num]\n",
    "        predict_num = predict_num.fillna(method='ffill').fillna(method='bfill')\n",
    "        predict_df.loc[predict_df.홍수사상번호 == num] = predict_num\n",
    "        \n",
    "def rmse(predict_df, name):\n",
    "    pred = predict_df[predict_df[f'pred_DNN_{name}'].notnull()][f'pred_DNN_{name}']\n",
    "    true = predict_df.loc[pred.index]['true']\n",
    "    return np.sqrt(mean_squared_error(true, pred))\n",
    "\n",
    "def rmsle(predict_df, name):\n",
    "    pred = predict_df[predict_df[f'pred_DNN_{name}'].notnull()][f'pred_DNN_{name}']\n",
    "    true = predict_df.loc[pred.index]['true']\n",
    "    return np.sqrt(mean_squared_log_error(true, pred))\n",
    "    \n",
    "def r2(predict_df, name):\n",
    "    pred = predict_df[predict_df[f'pred_DNN_{name}'].notnull()][f'pred_DNN_{name}']\n",
    "    true = predict_df.loc[pred.index]['true']\n",
    "    return r2_score(true, pred)\n",
    "\n",
    "def mape(predict_df, name):\n",
    "    pred = predict_df[predict_df[f'pred_DNN_{name}'].notnull()][f'pred_DNN_{name}']\n",
    "    true = predict_df.loc[pred.index]['true']\n",
    "    return np.mean(np.abs((true - pred) / true))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_structure = {\n",
    "    \"num_epochs\" : 50000,\n",
    "    \"learning_rate\" : 0.01,\n",
    "    \"hidden_layer\" : (1,1,1,1,1,1),\n",
    "    \"batch_norm\" : 1,\n",
    "    \"drop_out\" : 1,\n",
    "    \"drop_prob\" : 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 5\n",
    "name = f't + x2'\n",
    "\n",
    "data_t_x2 = data_raw.copy()\n",
    "\n",
    "x2_col = data_t_x2.filter(regex=\"수위\\(E지역\\)\").columns\n",
    "for col in x2_col:\n",
    "    data_t_x2[col + '_x2'] = data_t_x2[col] ** 2\n",
    "\n",
    "for col in data_t_x2.columns.difference([y_col] + PK_col):\n",
    "    data_t_x2[f'{col}_shift 1'] = data_t_x2[col].shift()\n",
    "    data_t_x2[f'{col}_shift1 -1'] = data_t_x2[col].shift(-1)\n",
    "\n",
    "shift_col = data_t_x2.filter(regex='shift').columns\n",
    "data_t_x2['홍수사상번호_shift 1'] = data_t_x2['홍수사상번호'].shift()\n",
    "data_t_x2['홍수사상번호_shift -1'] = data_t_x2['홍수사상번호'].shift(-1)\n",
    "\n",
    "data_t_x2.loc[(data_t_x2['홍수사상번호'] != data_t_x2['홍수사상번호_shift 1']), shift_col] = np.nan\n",
    "data_t_x2.loc[(data_t_x2['홍수사상번호'] != data_t_x2['홍수사상번호_shift -1']), shift_col] = np.nan\n",
    "\n",
    "train_valid_data = data_t_x2.dropna()\n",
    "train_data = train_valid_data.copy()\n",
    "test_data = data_t_x2.loc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "view_num = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = pd.DataFrame(columns=['RMSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hidden layer : [1] batch norm : 0 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 81.11 Valid Loss: 286.05\n",
      "\n",
      "hidden layer : [1] batch norm : 0 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 353.36 Valid Loss: 241.74\n",
      "\n",
      "hidden layer : [1] batch norm : 1 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 166.43 Valid Loss: 177.23\n",
      "\n",
      "hidden layer : [1] batch norm : 1 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 376.27 Valid Loss: 195.94\n",
      "\n",
      "hidden layer : [1, 1] batch norm : 0 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 38.79 Valid Loss: 285.84\n",
      "\n",
      "hidden layer : [1, 1] batch norm : 0 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 455.66 Valid Loss: 209.57\n",
      "\n",
      "hidden layer : [1, 1] batch norm : 1 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 26.58 Valid Loss: 250.19\n",
      "\n",
      "hidden layer : [1, 1] batch norm : 1 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 370.44 Valid Loss: 195.59\n",
      "\n",
      "hidden layer : [1, 1, 1] batch norm : 0 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 65.13 Valid Loss: 258.66\n",
      "\n",
      "hidden layer : [1, 1, 1] batch norm : 0 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 519.26 Valid Loss: 201.36\n",
      "\n",
      "hidden layer : [1, 1, 1] batch norm : 1 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 15.10 Valid Loss: 240.76\n",
      "\n",
      "hidden layer : [1, 1, 1] batch norm : 1 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 403.92 Valid Loss: 216.59\n",
      "\n",
      "hidden layer : [1, 1, 1, 1] batch norm : 0 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 84.58 Valid Loss: 239.04\n",
      "\n",
      "hidden layer : [1, 1, 1, 1] batch norm : 0 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 598.12 Valid Loss: 196.56\n",
      "\n",
      "hidden layer : [1, 1, 1, 1] batch norm : 1 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 12.70 Valid Loss: 245.81\n",
      "\n",
      "hidden layer : [1, 1, 1, 1] batch norm : 1 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 427.80 Valid Loss: 266.75\n",
      "\n",
      "hidden layer : [2] batch norm : 0 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 56.11 Valid Loss: 287.33\n",
      "\n",
      "hidden layer : [2] batch norm : 0 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 273.31 Valid Loss: 240.90\n",
      "\n",
      "hidden layer : [2] batch norm : 1 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 149.85 Valid Loss: 187.39\n",
      "\n",
      "hidden layer : [2] batch norm : 1 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 283.36 Valid Loss: 202.39\n",
      "\n",
      "hidden layer : [2, 2] batch norm : 0 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 78.49 Valid Loss: 256.40\n",
      "\n",
      "hidden layer : [2, 2] batch norm : 0 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 354.96 Valid Loss: 213.63\n",
      "\n",
      "hidden layer : [2, 2] batch norm : 1 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 25.70 Valid Loss: 230.60\n",
      "\n",
      "hidden layer : [2, 2] batch norm : 1 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 285.90 Valid Loss: 203.36\n",
      "\n",
      "hidden layer : [2, 2, 2] batch norm : 0 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 26.85 Valid Loss: 227.51\n",
      "\n",
      "hidden layer : [2, 2, 2] batch norm : 0 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 422.21 Valid Loss: 207.34\n",
      "\n",
      "hidden layer : [2, 2, 2] batch norm : 1 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 15.11 Valid Loss: 230.48\n",
      "\n",
      "hidden layer : [2, 2, 2] batch norm : 1 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 283.36 Valid Loss: 236.62\n",
      "\n",
      "hidden layer : [2, 2, 2, 2] batch norm : 0 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 32.62 Valid Loss: 240.37\n",
      "\n",
      "hidden layer : [2, 2, 2, 2] batch norm : 0 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 551.71 Valid Loss: 204.68\n",
      "\n",
      "hidden layer : [2, 2, 2, 2] batch norm : 1 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 7.83 Valid Loss: 242.09\n",
      "\n",
      "hidden layer : [2, 2, 2, 2] batch norm : 1 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 301.72 Valid Loss: 284.90\n",
      "\n",
      "hidden layer : [0.5] batch norm : 0 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 113.33 Valid Loss: 296.80\n",
      "\n",
      "hidden layer : [0.5] batch norm : 0 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 457.91 Valid Loss: 236.80\n",
      "\n",
      "hidden layer : [0.5] batch norm : 1 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 182.74 Valid Loss: 173.98\n",
      "\n",
      "hidden layer : [0.5] batch norm : 1 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 461.69 Valid Loss: 192.16\n",
      "\n",
      "hidden layer : [0.5, 0.5] batch norm : 0 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 45.07 Valid Loss: 315.02\n",
      "\n",
      "hidden layer : [0.5, 0.5] batch norm : 0 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 582.78 Valid Loss: 218.93\n",
      "\n",
      "hidden layer : [0.5, 0.5] batch norm : 1 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 38.11 Valid Loss: 264.65\n",
      "\n",
      "hidden layer : [0.5, 0.5] batch norm : 1 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 502.57 Valid Loss: 183.28\n",
      "\n",
      "hidden layer : [0.5, 0.5, 0.5] batch norm : 0 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 62.61 Valid Loss: 281.99\n",
      "\n",
      "hidden layer : [0.5, 0.5, 0.5] batch norm : 0 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 682.71 Valid Loss: 225.46\n",
      "\n",
      "hidden layer : [0.5, 0.5, 0.5] batch norm : 1 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 14.55 Valid Loss: 295.97\n",
      "\n",
      "hidden layer : [0.5, 0.5, 0.5] batch norm : 1 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 555.92 Valid Loss: 210.27\n",
      "\n",
      "hidden layer : [0.5, 0.5, 0.5, 0.5] batch norm : 0 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 50.61 Valid Loss: 258.50\n",
      "\n",
      "hidden layer : [0.5, 0.5, 0.5, 0.5] batch norm : 0 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 761.46 Valid Loss: 240.38\n",
      "\n",
      "hidden layer : [0.5, 0.5, 0.5, 0.5] batch norm : 1 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 11.84 Valid Loss: 299.40\n",
      "\n",
      "hidden layer : [0.5, 0.5, 0.5, 0.5] batch norm : 1 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 572.32 Valid Loss: 241.89\n",
      "\n",
      "hidden layer : [0.2] batch norm : 0 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 175.24 Valid Loss: 288.84\n",
      "\n",
      "hidden layer : [0.2] batch norm : 0 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 674.76 Valid Loss: 245.96\n",
      "\n",
      "hidden layer : [0.2] batch norm : 1 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 201.11 Valid Loss: 173.32\n",
      "\n",
      "hidden layer : [0.2] batch norm : 1 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 661.90 Valid Loss: 191.53\n",
      "\n",
      "hidden layer : [0.2, 0.2] batch norm : 0 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 74.72 Valid Loss: 306.24\n",
      "\n",
      "hidden layer : [0.2, 0.2] batch norm : 0 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 862.21 Valid Loss: 238.52\n",
      "\n",
      "hidden layer : [0.2, 0.2] batch norm : 1 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 53.70 Valid Loss: 271.31\n",
      "\n",
      "hidden layer : [0.2, 0.2] batch norm : 1 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 756.93 Valid Loss: 164.49\n",
      "\n",
      "hidden layer : [0.2, 0.2, 0.2] batch norm : 0 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 66.02 Valid Loss: 290.43\n",
      "\n",
      "hidden layer : [0.2, 0.2, 0.2] batch norm : 0 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 944.86 Valid Loss: 315.31\n",
      "\n",
      "hidden layer : [0.2, 0.2, 0.2] batch norm : 1 drop out : 0\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 25.29 Valid Loss: 313.08\n",
      "\n",
      "hidden layer : [0.2, 0.2, 0.2] batch norm : 1 drop out : 1\n",
      "1 START\n",
      "2 START\n",
      "3 START\n",
      "4 START\n",
      "5 START\n",
      "Train Loss: 875.95 Valid Loss: 210.81\n"
     ]
    }
   ],
   "source": [
    "model_structure = {\n",
    "    \"num_epochs\" : 10000,\n",
    "    \"learning_rate\" : 0.01,\n",
    "    \"hidden_layer\" : (1,1,1,1,1,1),\n",
    "    \"batch_norm\" : 1,\n",
    "    \"drop_out\" : 1,\n",
    "    \"drop_prob\" : 0.5\n",
    "}\n",
    "hidden_layers = ([1,],[1,1],[1,1,1],[1,1,1,1],\n",
    "                 [2,],[2,2],[2,2,2],[2,2,2,2],\n",
    "                 [0.5,],[0.5, 0.5],[0.5,0.5,0.5],[0.5,0.5,0.5,0.5],\n",
    "                 [0.2,],[0.2,0.2],[0.2,0.2,0.2])\n",
    "batch_norms = [0,1]\n",
    "drop_outs = [0,1]\n",
    "\n",
    "cv_num = 5\n",
    "\n",
    "for hidden_layer in hidden_layers:\n",
    "    for batch_norm in batch_norms:\n",
    "        for drop_out in drop_outs:\n",
    "            print('')\n",
    "            print(f'hidden layer : {hidden_layer} batch norm : {batch_norm} drop out : {drop_out}')\n",
    "            model_structure['hidden_layer'] = hidden_layer\n",
    "            model_structure['batch_norm'] = batch_norm\n",
    "            model_structure['drop_out'] = drop_out\n",
    "\n",
    "            scaler = RobustScaler()\n",
    "            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device\n",
    "\n",
    "            valid_data = train_valid_data[train_valid_data.홍수사상번호 == 15]\n",
    "            train_data = train_valid_data[train_valid_data.홍수사상번호 != 15]\n",
    "\n",
    "            X_train = train_data.drop(columns=[y_col]+PK_col)\n",
    "            y_train = train_data[y_col]\n",
    "\n",
    "            X_valid = valid_data.drop(columns=[y_col]+PK_col)\n",
    "            y_valid = valid_data[y_col]\n",
    "\n",
    "            X_test = test_data.drop(columns=[y_col]+PK_col)\n",
    "            y_test = test_data[y_col]\n",
    "\n",
    "            scaler.fit(X_train)\n",
    "\n",
    "            X_train_tensors = torch.Tensor(scaler.transform(X_train)).to(device)\n",
    "            y_train_tensors = torch.Tensor(y_train.values).to(device)\n",
    "\n",
    "            X_valid_tensors = torch.Tensor(scaler.transform(X_valid)).to(device)\n",
    "            y_valid_tensors = torch.Tensor(y_valid.values).to(device)\n",
    "\n",
    "            X_test_tensors = torch.Tensor(scaler.transform(X_test)).to(device)\n",
    "            y_test_tensors = torch.Tensor(y_test.values).to(device)\n",
    "\n",
    "            input_size = X_train.shape[1]\n",
    "            \n",
    "            train_losses = []\n",
    "            best_losses = []\n",
    "            \n",
    "            for num in range(cv_num):\n",
    "                print(f\"{num+1} START\")\n",
    "                dnn = DNN(input_size, model_structure[\"hidden_layer\"], model_structure[\"batch_norm\"],\n",
    "                          model_structure[\"drop_out\"], model_structure[\"drop_prob\"]).to(device)\n",
    "\n",
    "                criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "                optimizer = torch.optim.Adam(dnn.parameters(), lr=model_structure[\"learning_rate\"])  # adam optimizer\n",
    "\n",
    "                best_loss = 1E+10\n",
    "                best_epoch = 0\n",
    "\n",
    "                for epoch in range(model_structure[\"num_epochs\"]):\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = dnn(X_train_tensors)\n",
    "                    loss = criterion(outputs.view(-1), y_train_tensors.view(-1))\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        dnn.eval()\n",
    "                        outputs_valid = dnn(X_valid_tensors)\n",
    "                        loss_valid = criterion(outputs_valid.view(-1), y_valid_tensors.view(-1))\n",
    "                        if loss_valid < best_loss:\n",
    "                            best_loss = loss_valid\n",
    "                            best_epoch = epoch\n",
    "                        dnn.train()\n",
    "                        \n",
    "                train_losses.append(loss.item())\n",
    "                best_losses.append(best_loss.item())\n",
    "                \n",
    "            train_loss = np.mean(train_losses)\n",
    "            best_loss = np.mean(best_losses)\n",
    "            print(\"Train Loss: %1.2f Valid Loss: %1.2f\" % (np.sqrt(train_loss), np.sqrt(best_loss)))\n",
    "            valid_df.loc[str(list(model_structure.values())),'RMSE'] = np.sqrt(best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.2, 0.2], 1, 1, 0.5]</th>\n",
       "      <td>164.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.2], 1, 0, 0.5]</th>\n",
       "      <td>173.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.5], 1, 0, 0.5]</th>\n",
       "      <td>173.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [1], 1, 0, 0.5]</th>\n",
       "      <td>177.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.5, 0.5], 1, 1, 0.5]</th>\n",
       "      <td>183.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [2], 1, 0, 0.5]</th>\n",
       "      <td>187.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.2], 1, 1, 0.5]</th>\n",
       "      <td>191.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.5], 1, 1, 0.5]</th>\n",
       "      <td>192.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [1, 1], 1, 1, 0.5]</th>\n",
       "      <td>195.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [1], 1, 1, 0.5]</th>\n",
       "      <td>195.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [1, 1, 1, 1], 0, 1, 0.5]</th>\n",
       "      <td>196.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [1, 1, 1], 0, 1, 0.5]</th>\n",
       "      <td>201.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [2], 1, 1, 0.5]</th>\n",
       "      <td>202.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [2, 2], 1, 1, 0.5]</th>\n",
       "      <td>203.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [2, 2, 2, 2], 0, 1, 0.5]</th>\n",
       "      <td>204.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [2, 2, 2], 0, 1, 0.5]</th>\n",
       "      <td>207.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [1, 1], 0, 1, 0.5]</th>\n",
       "      <td>209.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.5, 0.5, 0.5], 1, 1, 0.5]</th>\n",
       "      <td>210.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.2, 0.2, 0.2], 1, 1, 0.5]</th>\n",
       "      <td>210.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [2, 2], 0, 1, 0.5]</th>\n",
       "      <td>213.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [1, 1, 1], 1, 1, 0.5]</th>\n",
       "      <td>216.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.5, 0.5], 0, 1, 0.5]</th>\n",
       "      <td>218.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.5, 0.5, 0.5], 0, 1, 0.5]</th>\n",
       "      <td>225.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [2, 2, 2], 0, 0, 0.5]</th>\n",
       "      <td>227.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [2, 2, 2], 1, 0, 0.5]</th>\n",
       "      <td>230.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [2, 2], 1, 0, 0.5]</th>\n",
       "      <td>230.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [2, 2, 2], 1, 1, 0.5]</th>\n",
       "      <td>236.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.5], 0, 1, 0.5]</th>\n",
       "      <td>236.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.2, 0.2], 0, 1, 0.5]</th>\n",
       "      <td>238.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [1, 1, 1, 1], 0, 0, 0.5]</th>\n",
       "      <td>239.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [2, 2, 2, 2], 0, 0, 0.5]</th>\n",
       "      <td>240.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.5, 0.5, 0.5, 0.5], 0, 1, 0.5]</th>\n",
       "      <td>240.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [1, 1, 1], 1, 0, 0.5]</th>\n",
       "      <td>240.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [2], 0, 1, 0.5]</th>\n",
       "      <td>240.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [1], 0, 1, 0.5]</th>\n",
       "      <td>241.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.5, 0.5, 0.5, 0.5], 1, 1, 0.5]</th>\n",
       "      <td>241.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [2, 2, 2, 2], 1, 0, 0.5]</th>\n",
       "      <td>242.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [1, 1, 1, 1], 1, 0, 0.5]</th>\n",
       "      <td>245.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.2], 0, 1, 0.5]</th>\n",
       "      <td>245.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [1, 1], 1, 0, 0.5]</th>\n",
       "      <td>250.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [2, 2], 0, 0, 0.5]</th>\n",
       "      <td>256.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.5, 0.5, 0.5, 0.5], 0, 0, 0.5]</th>\n",
       "      <td>258.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [1, 1, 1], 0, 0, 0.5]</th>\n",
       "      <td>258.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.5, 0.5], 1, 0, 0.5]</th>\n",
       "      <td>264.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [1, 1, 1, 1], 1, 1, 0.5]</th>\n",
       "      <td>266.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.2, 0.2], 1, 0, 0.5]</th>\n",
       "      <td>271.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.5, 0.5, 0.5], 0, 0, 0.5]</th>\n",
       "      <td>281.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [2, 2, 2, 2], 1, 1, 0.5]</th>\n",
       "      <td>284.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [1, 1], 0, 0, 0.5]</th>\n",
       "      <td>285.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [1], 0, 0, 0.5]</th>\n",
       "      <td>286.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [2], 0, 0, 0.5]</th>\n",
       "      <td>287.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.2], 0, 0, 0.5]</th>\n",
       "      <td>288.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.2, 0.2, 0.2], 0, 0, 0.5]</th>\n",
       "      <td>290.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.5, 0.5, 0.5], 1, 0, 0.5]</th>\n",
       "      <td>295.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.5], 0, 0, 0.5]</th>\n",
       "      <td>296.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.5, 0.5, 0.5, 0.5], 1, 0, 0.5]</th>\n",
       "      <td>299.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.2, 0.2], 0, 0, 0.5]</th>\n",
       "      <td>306.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.2, 0.2, 0.2], 1, 0, 0.5]</th>\n",
       "      <td>313.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.5, 0.5], 0, 0, 0.5]</th>\n",
       "      <td>315.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[10000, 0.01, [0.2, 0.2, 0.2], 0, 1, 0.5]</th>\n",
       "      <td>315.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 RMSE\n",
       "[10000, 0.01, [0.2, 0.2], 1, 1, 0.5]           164.49\n",
       "[10000, 0.01, [0.2], 1, 0, 0.5]                173.32\n",
       "[10000, 0.01, [0.5], 1, 0, 0.5]                173.98\n",
       "[10000, 0.01, [1], 1, 0, 0.5]                  177.23\n",
       "[10000, 0.01, [0.5, 0.5], 1, 1, 0.5]           183.28\n",
       "[10000, 0.01, [2], 1, 0, 0.5]                  187.39\n",
       "[10000, 0.01, [0.2], 1, 1, 0.5]                191.53\n",
       "[10000, 0.01, [0.5], 1, 1, 0.5]                192.16\n",
       "[10000, 0.01, [1, 1], 1, 1, 0.5]               195.59\n",
       "[10000, 0.01, [1], 1, 1, 0.5]                  195.94\n",
       "[10000, 0.01, [1, 1, 1, 1], 0, 1, 0.5]         196.56\n",
       "[10000, 0.01, [1, 1, 1], 0, 1, 0.5]            201.36\n",
       "[10000, 0.01, [2], 1, 1, 0.5]                  202.39\n",
       "[10000, 0.01, [2, 2], 1, 1, 0.5]               203.36\n",
       "[10000, 0.01, [2, 2, 2, 2], 0, 1, 0.5]         204.68\n",
       "[10000, 0.01, [2, 2, 2], 0, 1, 0.5]            207.34\n",
       "[10000, 0.01, [1, 1], 0, 1, 0.5]               209.57\n",
       "[10000, 0.01, [0.5, 0.5, 0.5], 1, 1, 0.5]      210.27\n",
       "[10000, 0.01, [0.2, 0.2, 0.2], 1, 1, 0.5]      210.81\n",
       "[10000, 0.01, [2, 2], 0, 1, 0.5]               213.63\n",
       "[10000, 0.01, [1, 1, 1], 1, 1, 0.5]            216.59\n",
       "[10000, 0.01, [0.5, 0.5], 0, 1, 0.5]           218.93\n",
       "[10000, 0.01, [0.5, 0.5, 0.5], 0, 1, 0.5]      225.46\n",
       "[10000, 0.01, [2, 2, 2], 0, 0, 0.5]            227.51\n",
       "[10000, 0.01, [2, 2, 2], 1, 0, 0.5]            230.48\n",
       "[10000, 0.01, [2, 2], 1, 0, 0.5]               230.60\n",
       "[10000, 0.01, [2, 2, 2], 1, 1, 0.5]            236.62\n",
       "[10000, 0.01, [0.5], 0, 1, 0.5]                236.80\n",
       "[10000, 0.01, [0.2, 0.2], 0, 1, 0.5]           238.52\n",
       "[10000, 0.01, [1, 1, 1, 1], 0, 0, 0.5]         239.04\n",
       "[10000, 0.01, [2, 2, 2, 2], 0, 0, 0.5]         240.37\n",
       "[10000, 0.01, [0.5, 0.5, 0.5, 0.5], 0, 1, 0.5] 240.38\n",
       "[10000, 0.01, [1, 1, 1], 1, 0, 0.5]            240.76\n",
       "[10000, 0.01, [2], 0, 1, 0.5]                  240.90\n",
       "[10000, 0.01, [1], 0, 1, 0.5]                  241.74\n",
       "[10000, 0.01, [0.5, 0.5, 0.5, 0.5], 1, 1, 0.5] 241.89\n",
       "[10000, 0.01, [2, 2, 2, 2], 1, 0, 0.5]         242.09\n",
       "[10000, 0.01, [1, 1, 1, 1], 1, 0, 0.5]         245.81\n",
       "[10000, 0.01, [0.2], 0, 1, 0.5]                245.96\n",
       "[10000, 0.01, [1, 1], 1, 0, 0.5]               250.19\n",
       "[10000, 0.01, [2, 2], 0, 0, 0.5]               256.40\n",
       "[10000, 0.01, [0.5, 0.5, 0.5, 0.5], 0, 0, 0.5] 258.50\n",
       "[10000, 0.01, [1, 1, 1], 0, 0, 0.5]            258.66\n",
       "[10000, 0.01, [0.5, 0.5], 1, 0, 0.5]           264.65\n",
       "[10000, 0.01, [1, 1, 1, 1], 1, 1, 0.5]         266.75\n",
       "[10000, 0.01, [0.2, 0.2], 1, 0, 0.5]           271.31\n",
       "[10000, 0.01, [0.5, 0.5, 0.5], 0, 0, 0.5]      281.99\n",
       "[10000, 0.01, [2, 2, 2, 2], 1, 1, 0.5]         284.90\n",
       "[10000, 0.01, [1, 1], 0, 0, 0.5]               285.84\n",
       "[10000, 0.01, [1], 0, 0, 0.5]                  286.05\n",
       "[10000, 0.01, [2], 0, 0, 0.5]                  287.33\n",
       "[10000, 0.01, [0.2], 0, 0, 0.5]                288.84\n",
       "[10000, 0.01, [0.2, 0.2, 0.2], 0, 0, 0.5]      290.43\n",
       "[10000, 0.01, [0.5, 0.5, 0.5], 1, 0, 0.5]      295.97\n",
       "[10000, 0.01, [0.5], 0, 0, 0.5]                296.80\n",
       "[10000, 0.01, [0.5, 0.5, 0.5, 0.5], 1, 0, 0.5] 299.40\n",
       "[10000, 0.01, [0.2, 0.2], 0, 0, 0.5]           306.24\n",
       "[10000, 0.01, [0.2, 0.2, 0.2], 1, 0, 0.5]      313.08\n",
       "[10000, 0.01, [0.5, 0.5], 0, 0, 0.5]           315.02\n",
       "[10000, 0.01, [0.2, 0.2, 0.2], 0, 1, 0.5]      315.31"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.sort_values('RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000, Train Loss: 857.52 Best Epoch: 912, Valid Loss: 203.53\n",
      "Epoch: 2000, Train Loss: 835.10 Best Epoch: 1210, Valid Loss: 171.81\n",
      "Epoch: 3000, Train Loss: 830.44 Best Epoch: 2710, Valid Loss: 171.41\n",
      "Epoch: 4000, Train Loss: 849.48 Best Epoch: 3308, Valid Loss: 164.86\n",
      "Epoch: 5000, Train Loss: 776.97 Best Epoch: 3308, Valid Loss: 164.86\n",
      "Epoch: 6000, Train Loss: 780.89 Best Epoch: 3308, Valid Loss: 164.86\n",
      "Epoch: 7000, Train Loss: 804.84 Best Epoch: 6898, Valid Loss: 163.68\n",
      "Epoch: 8000, Train Loss: 780.46 Best Epoch: 6898, Valid Loss: 163.68\n",
      "Epoch: 9000, Train Loss: 813.80 Best Epoch: 6898, Valid Loss: 163.68\n",
      "Epoch: 10000, Train Loss: 815.76 Best Epoch: 6898, Valid Loss: 163.68\n"
     ]
    }
   ],
   "source": [
    "model_structure = {\n",
    "    \"num_epochs\" : 10000,\n",
    "    \"learning_rate\" : 0.01,\n",
    "    \"hidden_layer\" : (0.2, 0.2),\n",
    "    \"batch_norm\" : 1,\n",
    "    \"drop_out\" : 1,\n",
    "    \"drop_prob\" : 0.5\n",
    "}\n",
    "\n",
    "scaler = RobustScaler()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # device\n",
    "\n",
    "valid_data = train_valid_data[train_valid_data.홍수사상번호 == 15]\n",
    "train_data = train_valid_data[train_valid_data.홍수사상번호 != 15]\n",
    "\n",
    "X_train = train_data.drop(columns=[y_col]+PK_col)\n",
    "y_train = train_data[y_col]\n",
    "\n",
    "X_valid = valid_data.drop(columns=[y_col]+PK_col)\n",
    "y_valid = valid_data[y_col]\n",
    "\n",
    "X_test = test_data.drop(columns=[y_col]+PK_col)\n",
    "y_test = test_data[y_col]\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_tensors = torch.Tensor(scaler.transform(X_train)).to(device)\n",
    "y_train_tensors = torch.Tensor(y_train.values).to(device)\n",
    "\n",
    "X_valid_tensors = torch.Tensor(scaler.transform(X_valid)).to(device)\n",
    "y_valid_tensors = torch.Tensor(y_valid.values).to(device)\n",
    "\n",
    "X_test_tensors = torch.Tensor(scaler.transform(X_test)).to(device)\n",
    "y_test_tensors = torch.Tensor(y_test.values).to(device)\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "dnn = DNN(input_size, model_structure[\"hidden_layer\"], model_structure[\"batch_norm\"],\n",
    "          model_structure[\"drop_out\"], model_structure[\"drop_prob\"]).to(device)\n",
    "\n",
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(dnn.parameters(), lr=model_structure[\"learning_rate\"])  # adam optimizer\n",
    "\n",
    "best_loss = 1E+10\n",
    "best_epoch = 0\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(model_structure[\"num_epochs\"]):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = dnn(X_train_tensors)\n",
    "    loss = criterion(outputs.view(-1), y_train_tensors.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        dnn.eval()\n",
    "        outputs_valid = dnn(X_valid_tensors)\n",
    "        loss_valid = criterion(outputs_valid.view(-1), y_valid_tensors.view(-1))\n",
    "        valid_losses.append(loss_valid.item())\n",
    "        if loss_valid < best_loss:\n",
    "            best_loss = loss_valid\n",
    "            best_epoch = epoch\n",
    "            pred_test = dnn(X_test_tensors).view(-1).detach().cpu().numpy()\n",
    "            pred_test[0] = pred_test[1]\n",
    "            pred_test[-1] = pred_test[-2]\n",
    "        dnn.train()\n",
    "\n",
    "    if ((epoch+1) % (model_structure[\"num_epochs\"]//view_num) == 0):\n",
    "        print(\"Epoch: %d, Train Loss: %1.2f Best Epoch: %d, Valid Loss: %1.2f\" % (epoch+1, np.sqrt(loss.item()), best_epoch+1, np.sqrt(best_loss.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNN(\n",
       "  (hidden_layer): Sequential(\n",
       "    (0): Linear(in_features=146, out_features=29, bias=True)\n",
       "    (1): BatchNorm1d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=29, out_features=29, bias=True)\n",
       "    (5): BatchNorm1d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Dropout(p=0.5, inplace=False)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Linear(in_features=29, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1 = plt.subplots(1,1)\n",
    "fig.set_figheight(4)\n",
    "fig.set_figwidth(12)\n",
    "\n",
    "ax1.plot(pred_test,color='dodgerblue', label='유입량')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(test_data['데이터집단 1_수위(E지역)'].values,color='limegreen', label='수위(E지역)')\n",
    "ax1.set_ylabel('유입량')\n",
    "ax2.set_ylabel('수위(E지역)')\n",
    "ax1.legend(loc=2)\n",
    "ax2.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_excel('../data/02_평가데이터/데이터분석분야_퓨처스리그_홍수ZERO_홍수빅타_평가데이터.xlsx')\n",
    "test_df.loc[1:,'유입량'] = pred_test\n",
    "test_df = test_df.loc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NO</th>\n",
       "      <th>홍수사상번호</th>\n",
       "      <th>연</th>\n",
       "      <th>월</th>\n",
       "      <th>일</th>\n",
       "      <th>시간</th>\n",
       "      <th>유입량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2018.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>253.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2018.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>253.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2018.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>292.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2018.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>281.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2018.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>288.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>156.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2018.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>17.00</td>\n",
       "      <td>420.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>157.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2018.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>419.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>158.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2018.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>19.00</td>\n",
       "      <td>417.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>159.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2018.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>414.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>160.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>2018.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>21.00</td>\n",
       "      <td>414.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        NO  홍수사상번호       연    월    일    시간    유입량\n",
       "1     1.00   26.00 2018.00 7.00 1.00  6.00 253.74\n",
       "2     2.00   26.00 2018.00 7.00 1.00  7.00 253.74\n",
       "3     3.00   26.00 2018.00 7.00 1.00  8.00 292.10\n",
       "4     4.00   26.00 2018.00 7.00 1.00  9.00 281.16\n",
       "5     5.00   26.00 2018.00 7.00 1.00 10.00 288.38\n",
       "..     ...     ...     ...  ...  ...   ...    ...\n",
       "156 156.00   26.00 2018.00 7.00 7.00 17.00 420.27\n",
       "157 157.00   26.00 2018.00 7.00 7.00 18.00 419.39\n",
       "158 158.00   26.00 2018.00 7.00 7.00 19.00 417.26\n",
       "159 159.00   26.00 2018.00 7.00 7.00 20.00 414.63\n",
       "160 160.00   26.00 2018.00 7.00 7.00 21.00 414.63\n",
       "\n",
       "[160 rows x 7 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('../data/02_평가데이터/predict_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
